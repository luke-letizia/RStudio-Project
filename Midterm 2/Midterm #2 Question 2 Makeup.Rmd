---
title: 'Midterm #2 Question 2 makeup'
author: "Luke Letizia"
date: "May 3, 2018"
output: html_document
---

##Question 2##

###Introduction
The car insurance company has already implemented a poisson regression model to describe the claim counts distribution. I have been hired to test whether the poisson model should be replaced by a homogeneous negative binomial model. I have concluded that a Negative Binomial model better fits the data and should be used to describe the claim counts distribution.

###Methods
Linders Consulting is implementing a poisson regression model to describe the claim counts distribution. Because $\lambda$ is the parameter of a poisson distribution and is the value for both the mean and variance, it may be inaccurate to assume all samples follow the same $\lambda$ variable given different people have different behaviors and different risk factors. When the mean and variance are not equal in a counts distribution, there is an under or overdispersion, depending on if the variance is smaller or larger than the mean. The issue could be resolved by implementing a homogeneous negative binomial regression model. The variance is $\lambda(1+\lambda/\gamma)$ and mean of $\lambda$. After implementing the negative binomial and finding the maximum predicted log likelihood $\lambda$ and $\gamma$, I used the persp plotting function to prove the predicted values were the absolute maximums. Another method used to conclude the negative binomial was comparing the chi-square statistics of the negative binomial and the possion regression.

###Results
By comparing the poisson regression and the negative binomial, I have concluded that the negative binomial is favored over the poisson because the predicted values are much closer and more honed in to the actual data set than the poisson model is. The chi-square statistic for the negative binomial is also much closer to 0 which proves it is superior. 

###Conclusion
Through the methods used and based on the statistical proof from the dataset provided, I conlcude that the homogeneous negative should replace the poisson regression model. By using the negative binomial, future premiums should thus decrease becuase it increases the predicted number that are accident free and decreases the number that have one or more accidents. 


###Implementations
```{r, echo = FALSE, results = 'hide'}
Data = read.csv("C:/Users/Luke\\Downloads/SingaporeAuto.csv")
```

The summary of the data doesn't really give us much information to go off of for the claim counts so we look at the histogram to get a better visualization of the data.

```{r, echo = FALSE, results = 'hide'}
n=length(Data$Clm_Count)


head(Data)
summary(Data$Clm_Count)
summary(Data$Exp_weights)
par(mfrow=c(1,2))
```

```{r, echo = FALSE}
H=hist(Data$Clm_Count,5, main="Claims Count")
hist(Data$Exp_weights, main="Exposures")
```

```{r, echo = FALSE}
# Claim counts
NrClaims=unique(Data$Clm_Count) 
Claims=c()
nn=length(NrClaims)
for(i in 1:nn){
Claims[i]=sum(Data$Clm_Count==NrClaims[i])
}
A=as.table(matrix(c(NrClaims, Claims), length(Claims),2))
colnames(A)=c("#Claims", "Observed")
rownames(A)=c("","","","")
A
```

###Implementing a Homogeneous Poisson Model
```{r, echo = FALSE}
# Fit Poisson distribution with and wihtout Exposures
mu=mean(Data$Clm_Count)
mu_E=sum(Data$Clm_Count)/sum(Data$Exp_weights)
Claims_Poisson=c()
Claims_Poisson_E=c()
for(i in 1:nn){
  k=NrClaims[i]
  Claims_Poisson[i]=n*exp(-mu)*mu^k/factorial(k)
  Claims_Poisson_E[i]=sum(exp(-mu_E*Data$Exp_weights)*(mu_E*Data$Exp_weights)^k/factorial(k))
}
options(digits=2)
Table=matrix(c(NrClaims, Claims,Claims_Poisson,Claims_Poisson_E), length(Claims),4)
colnames(Table)=c("#Claims", "Data","Model without Exposure","Model With Exposure"  )
rownames(Table)=c("","","","")
as.table(Table)

ChiSqSt=sum((Claims-Claims_Poisson)^2/Claims_Poisson)
ChiSqSt_E=sum((Claims-Claims_Poisson_E)^2/Claims_Poisson_E)
```

We can see that the model without exposure differs from the actual data observed. We can see that including exposures is necesarry for the model because it hones the values for each claim count closer to the observed values, but it is still not an adeqaute fit.

#####Chi Square Statistic without exposures:

```{r, echo = FALSE}
ChiSqSt
```

#####Chi Square Statistic with exposures:

```{r, echo = FALSE}
ChiSqSt_E
```

Because the Chi-Square Satistic is much lower with exposures, we can see that Exposures are an important factor to include when modeling the data.

###Generalized Linear Poisson Model

We now investigate modeling different risk parameters for different drivers based on factors involved.

```{r, echo = FALSE, results = 'hide'}
#####Investigate personal characteristics
Data$Female=1*(Data$SexInsured=="F")
Data$Auto=1*(Data$VehicleType=="A")
Data$NCD1F=relevel(factor(Data$NCD), ref="50")
I=(Data$AgeCat==0)
Data$AgeCat[I]=7
Data$AgeCatF=relevel(factor(Data$AgeCat), ref="7")
Data$VAgeCat1F=relevel(factor(Data$VAgecat1), ref="6")

poisson =glm(Clm_Count ~ Female + Auto  + Auto:AgeCatF + Auto:NCD1F + VAgeCat1F, offset=LNWEIGHT, poisson(link = log), data = Data)
```

```{r, echo = FALSE, results = 'hide'}
#####Split data in Auto and Other
Data_Auto=Data[Data$VehicleType=="A",]
Data_Other=Data[Data$VehicleType!="A",]
n_Auto=length(Data_Auto$Clm_Count)
n_Other=length(Data_Other$Clm_Count)

Nr_Acc_Auto=sum(Data_Auto$Clm_Count)
Nr_Acc_Other=sum(Data_Other$Clm_Count)
Pr_Acc_Auto=Nr_Acc_Auto/n_Auto
Pr_Acc_Other=Nr_Acc_Other/n_Other
```

```{r, echo = FALSE, results = 'hide'}
NrClaims=unique(Data$Clm_Count) 
Claims_Auto=c()
Claims_Other=c()
nn=length(NrClaims)
for(i in 1:nn){
  Claims_Auto[i]=100*sum(Data_Auto$Clm_Count==NrClaims[i])/n_Auto
  Claims_Other[i]=100*sum(Data_Other$Clm_Count==NrClaims[i])/n_Other
}
A=as.table(matrix(c(NrClaims, Claims_Auto, Claims_Other), length(Claims_Auto),3))
colnames(A)=c("#Claims", "Auto", "Other")
rownames(A)=c("","","","")
A
```

```{r, echo = FALSE, results = 'hide'}
NrClaims=unique(Data$Clm_Count) 
Claims_VCat1=c()
Claims_VCat2=c()
Claims_VCat3=c()
Claims_VCat4=c()
Claims_VCat5=c()
nn=length(NrClaims)

for(i in 1:nn){
  Claims_VCat1[i]=100*sum(Data$Clm_Count[Data$VAgeCat1==2]==NrClaims[i])/sum(Data$VAgecat1==2)
  Claims_VCat2[i]=100*sum(Data$Clm_Count[Data$VAgeCat1==3]==NrClaims[i])/sum(Data$VAgecat1==3)
  Claims_VCat3[i]=100*sum(Data$Clm_Count[Data$VAgeCat1==4]==NrClaims[i])/sum(Data$VAgecat1==4)
  Claims_VCat4[i]=100*sum(Data$Clm_Count[Data$VAgeCat1==5]==NrClaims[i])/sum(Data$VAgecat1==5)
  Claims_VCat5[i]=100*sum(Data$Clm_Count[Data$VAgeCat1==6]==NrClaims[i])/sum(Data$VAgecat1==6)
 }
A=as.table(matrix(c(NrClaims,  Claims_VCat1,  Claims_VCat2,  Claims_VCat3,  Claims_VCat4,  Claims_VCat5),  length( Claims_VCat1),6))
colnames(A)=c("#Claims", "Age Cat 1: 0-2", "Age Cat 2: 3-5", "Age Cat 3: 6-10", "Age Cat 4: 11-15", "Age Cat 5: 16 and older")
rownames(A)=c()
A
```

```{r, echo = FALSE, results = 'hide'}
NrClaims=unique(Data$Clm_Count) 
Claims_AgeCat1=c()
Claims_AgeCat2=c()
Claims_AgeCat3=c()
Claims_AgeCat4=c()
Claims_AgeCat5=c()
Claims_AgeCat6=c()

nn=length(NrClaims)

for(i in 1:nn){
  Claims_AgeCat1[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$AgeCat==2]==NrClaims[i])/sum(Data_Auto$AgeCat==2)
  Claims_AgeCat2[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$AgeCat==3]==NrClaims[i])/sum(Data_Auto$AgeCat==3)
  Claims_AgeCat3[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$AgeCat==4]==NrClaims[i])/sum(Data_Auto$AgeCat==4)
  Claims_AgeCat4[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$AgeCat==5]==NrClaims[i])/sum(Data_Auto$AgeCat==5)
  Claims_AgeCat5[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$AgeCat==6]==NrClaims[i])/sum(Data_Auto$AgeCat==6)
  Claims_AgeCat6[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$AgeCat==7]==NrClaims[i])/sum(Data_Auto$AgeCat==7)
}
A=as.table(matrix(c(NrClaims,  Claims_AgeCat1,  Claims_AgeCat2,  Claims_AgeCat3,  Claims_AgeCat4,  Claims_AgeCat5, Claims_AgeCat6),  length( Claims_AgeCat1),7))
colnames(A)=c("#Claims", "Age Cat 1: 22-25", "Age Cat 2: 26-35", "Age Cat 3: 36-45", "Age Cat 4: 46-55", "Age Cat 5: 56-65", "Age Cat 6: 66 and over")
rownames(A)=c()
A
```

```{r, echo = FALSE, results = 'hide'}
NrClaims=unique(Data$Clm_Count) 
Claims_NCD1=c()
Claims_NCD2=c()
Claims_NCD3=c()
Claims_NCD4=c()
Claims_NCD5=c()
Claims_NCD6=c()

nn=length(NrClaims)

for(i in 1:nn){
  Claims_NCD1[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$NCD==0]==NrClaims[i])/sum(Data_Auto$NCD==0)
  Claims_NCD2[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$NCD==10]==NrClaims[i])/sum(Data_Auto$NCD==10)
  Claims_NCD3[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$NCD==20]==NrClaims[i])/sum(Data_Auto$NCD==20)
  Claims_NCD4[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$NCD==30]==NrClaims[i])/sum(Data_Auto$NCD==30)
  Claims_NCD5[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$NCD==40]==NrClaims[i])/sum(Data_Auto$NCD==40)
  Claims_NCD6[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$NCD==50]==NrClaims[i])/sum(Data_Auto$NCD==50)
}
A=as.table(matrix(c(NrClaims,  Claims_NCD1,  Claims_NCD2,  Claims_NCD3,  Claims_NCD4,  Claims_NCD5, Claims_NCD6),  length( Claims_AgeCat1),7))
colnames(A)=c("#Claims", "0", "10", "20", "30", "40", "50")
rownames(A)=c()
A
```

```{r, echo = FALSE, results = 'hide'}
CountGLM_P=glm(Clm_Count ~ Female + Auto  + Auto:AgeCatF + Auto:NCD1F + VAgeCat1F, offset=LNWEIGHT, poisson(link=log), data= Data)
```

####Comparing the Homogeneous Poisson and Poisson Regression
```{r, echo = FALSE}
NrClaims=unique(Data$Clm_Count) 

mu=mean(Data$Clm_Count)
mu_E=sum(Data$Clm_Count)/sum(Data$Exp_weights)
lambdaHat_GLM=exp(predict(CountGLM_P))

Claims=c()
Claims_Poisson=c()
Claims_Poisson_E=c()
Claims_Poisson_GLM=c()

for(i in 1:length(NrClaims)){
  k=NrClaims[i]
  Claims[i]=sum(Data$Clm_Count==NrClaims[i])
  
  Claims_Poisson[i]=n*exp(-mu)*mu^k/factorial(k)
  Claims_Poisson_E[i]=sum(exp(-mu_E*Data$Exp_weights)*(mu_E*Data$Exp_weights)^k/factorial(k))
  Claims_Poisson_GLM[i]=sum(exp(-lambdaHat_GLM)*(lambdaHat_GLM)^k/factorial(k))
}
options(digits=2)
Table=matrix(c(NrClaims, Claims,Claims_Poisson,Claims_Poisson_E,  Claims_Poisson_GLM), length(Claims),5)
colnames(Table)=c("#Claims", "Data","Model without Exposure","Model With Exposure", "Poisson Regression"  )
rownames(Table) = c("", "", "", "")
as.table(Table)
```

###Fit Negative Binomial
```{r, echo = FALSE, results = 'hide'}
functn = function(Par, N, E) {
  lamb = Par[1]
  gam = Par[2]
NB = -sum(log((gamma(N+gam))/(gamma(gam)*factorial(N))*((1-((lamb * E)/(gam + (lamb * E))))^gam)*(((lamb * E)/(gam + (lamb * E)))^N)))
return(NB)
}

param = c(1, 1)
functn(Par = param, N = Data$Clm_Count, E = Data$Exp_weights)
opt = optim(par = param, fn = functn, N = Data$Clm_Count, E = Data$Exp_weights)
opt
Lambda = opt$par[1]
Gamma = opt$par[2]
Lambda
Gamma
```

Max $\lambda$ = 0.13
Max $\gamma$ = 1.42

####Plotted Log Likelihood Function for different values of $\lambda$ and $\gamma$
```{r, echo = FALSE}
z = matrix(data = NA, nrow = 40, ncol = 40)
x = seq(0.01, 0.4, 0.01)
y = seq(.04, 1.6, 0.04)
for(i in 1:length(x)){
  for(j in 1:length(y)){
    vector = c(x[i], y[j])
  z[i, j] = functn(Par = vector, N = Data$Clm_Count, E =   Data$Exp_weights)
  }
}
persp(x, y, z)
##By the persp plot we can see that there is only in maximum and one minimum, so the optim values of the maximum values are the absolute maximums.
```


####Comparing the Negative Binomial and the Poisson Regression
```{r, echo = FALSE}
Claims_NB_E=c()
for(i in 1:length(NrClaims)){
  N = unique(NrClaims[i])
  Claims_NB_E[i]=sum((gamma(N+Gamma))/(gamma(Gamma)*factorial(N))*((1-((Lambda * Data$Exp_weights)/(Gamma + (Lambda * Data$Exp_weights))))^Gamma)*(((Lambda * Data$Exp_weights)/(Gamma + (Lambda * Data$Exp_weights)))^N))
  }

options(digits=2)
Table=matrix(c(NrClaims, Claims,Claims_NB_E), length(Claims),3)
colnames(Table)=c("#Claims", "Data", "Negative Binomial"  )
rownames(Table)=c("","","","")
as.table(Table)

## Comparing the models
options(digits=2)
Table=matrix(c(NrClaims, Claims, Claims_Poisson_GLM, Claims_NB_E), length(Claims),4)
colnames(Table)=c("#Claims", "Data", "Poisson Regression", "Negative Binomial"  )
rownames(Table) = c("", "", "", "")
as.table(Table)
## ==> We can see that the Negative Binomial is much more precise and closer to the actual data values than the Poisson Regression model is.
```

####Negative Binomial Chi-Square Statistic
```{r, echo = FALSE}
##Chi Squares
ChiSqSt_NB_E=sum((Claims-Claims_NB_E)^2/Claims_NB_E)
ChiSqSt_NB_E
```

####Poisson Regression Chi-Square Statistic
```{r, echo = FALSE}
ChiSqST_Poisson_GLM=sum((Claims-Claims_Poisson_GLM)^2/Claims_Poisson_GLM)
ChiSqST_Poisson_GLM
## ==> The Chi Square statistic for the Negative Binomial is much smaller than the Poisson Regression model's statistic which is evidence that the Negative Binomial differs much less from the actual data than Poisson.
```

###Proofs
$E[N] = E[E[N|\theta]] = E[\theta\lambda] = \lambda E[\theta] = \lambda*1 = \lambda$

$Var[N] = E[Var[N|\theta]] + Var[E[N|\theta]] = E[Var[\theta\lambda]] + Var[E[[\theta\lambda]] = \lambda^21/\gamma + Var[\lambda] = \lambda^2 1/\gamma + \lambda = \lambda +(1 + \lambda/\gamma)$

Overdispersion is when there is a presence of a greater variability than should be expected. As we can see with the Negative Binomial, it has a smaller dispersion and thus is favored over Poisson.