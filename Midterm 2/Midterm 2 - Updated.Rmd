---
title: "Midterm 2"
author: "Luke Letizia"
date: "April 8, 2018"
output: html_document
---

###Question 1

A.

$\text{L} = \prod_{i=1}^ne^{\frac{y_i\theta_i-\text{b}(\theta_i)}{\phi_i}+\text{S}(y_i,\phi_i)}$

-I started the maximum likelihood procedure by taking the product of the function.

$\text{ln(L)} = \sum_{i=1}^n\frac{y_i\theta_i-\text{b}(\theta_i)}{\phi_i}+\text{S}(y_i\phi_i)$

-I then took the natural log of the product.

$\frac{\delta\text{L}}{\delta\underline\beta} = \underline0 =  \sum_{i=1}^n\frac{(y_i-\text{b}^\prime(\theta_i)\frac{\delta\theta}{\delta\underline\beta})}{\phi_i}$

-Here I took the derivative of the function.

$=\sum_{i=1}^n\frac{y_i-\mu_i}{\text{Var[Y]}}*\text{b}^{\prime\prime}(\theta)*\frac{\delta\theta}{\delta\underline\beta}$

-Here I took the derivative of the function and then substituted the following values; $\phi = \frac{\text{Var[Y]}}{\text{b}^{\prime\prime}(\theta)}$ and $\text{b}^\prime(\theta) = \mu$

$=\sum_{i=1}^n\frac{y_i-\mu_i}{\text{Var[Y]}}(\frac{\delta\mu}{\delta\underline\beta})(\frac{\delta\underline\beta}{\delta\theta})*(\frac{\delta\theta}{\delta\underline\beta})$

-I also substituted the following value to solve for the simplest form; $\frac{\delta\mu}{\delta\underline\beta} = \frac{\delta\text{b}^\prime(\theta)}{\delta\underline\beta} = \text{b}^{\prime\prime}(\theta)\frac{\delta\theta}{\delta\underline\beta} <=> \text{b}^{\prime\prime}(\theta) = \frac{\delta\mu}{\delta\underline\beta} * \frac{\delta\underline\beta}{\delta\theta}$

$=\sum_{i=1}^n\frac{y_i-\mu_i}{\text{Var[Y]}}*\frac{\delta\mu}{\delta\underline\beta} = \underline0$

B.

$\text{L} = \prod_{i=1}^ne^{\frac{y_i\theta_i-\text{b}(\theta_i)}{\phi_i}+\text{S}(y_i,\phi_i)}$

-I started the maximum likelihood procedure by taking the product of the function.

$\text{ln(L)} = \sum_{i=1}^n\frac{y_i\theta_i-\text{b}(\theta_i)}{\phi_i}+\text{S}(y_i\phi_i)$

-I then took the natural log of the product.

$\frac{\delta\text{L}}{\delta\underline\beta} = 0 = \frac{\delta}{\delta\underline\beta_\text{L}}\sum_{i=1}^n \frac{y_i x_i^\text{T} \underline{\beta} - \text{b}^\prime(x_i^\text{T} \underline{\beta})}{\phi_i} + S(y_i, \phi_i)$ 

-Here I substituted $\theta_i$ values for $x_i^\text{T}\underline\beta$

$= \sum_{i=1}^n \frac{y_i x_{i\text{L}} - b^\prime(x_i^\text{T} \underline\beta x_{i\text{L}})}{\phi_i}$

-Here, I took the derivative of the log function.

$= \frac{1}{\phi}\sum_{i=1}^n (y_i - b^\prime(x_i^\text{T} \underline\beta)) x_{i\text{L}}w_i$

-I then substituted the following value and simplified; $\phi_i = \frac{\phi}{w_i}$

$= \sum_{i=1}^n (y_i - b^\prime(\underline\beta))*w_i$

-Setting $x_i = 1$ for $i = 1,2,...,n$, I got the following result.

$= \sum_{i=1}^n (y_i - \mu))*w_i = 0$

-I finally substituted $\text{b}^\prime(\underline\beta) = \mu$ and solve for $\hat{\mu}$

$\sum_{i=1}^n y_iw_i = \sum_{i=1}^n \mu*w_i <=> \hat{\mu} = \frac{\sum_{i=1}^ny_iw_i}{\sum_{i=1}^nw_i}$

C.

$g(\mu) = \theta$ & $\theta = \mu$

$\hat{\mu} = \hat{\mu} = \frac{\sum_{i=1}^ny_iw_i}{\sum_{i=1}^nw_i}$

-Here, I substituted  $\hat{\mu}$, which we found in part b with the value of $\hat{\lambda}$ which I found from a series of equivalences.

$= \frac{\sum_{i=1}^ny_i}{n} = \bar{y}$

-By setting $w_i = 1$ we could solve for $\hat{\mu} = \bar{y}$

D.

$g(\mu) = \theta$ & $\theta = \text{log}\lambda$

$\mu = \text{b}^\prime(\theta) = e^\theta = e^{\text{log}\lambda} = \lambda$

$\hat{\mu} = \hat{\lambda} = \frac{\sum_{i=1}^ny_iw_i}{\sum_{i=1}^nw_i}$

-Here, I substituted  $\hat{\mu}$, which we found in part b with the value of $\hat{\mu}$ which I found from a series of equivalences.

$= \frac{\sum_{i=1}^ny_i}{n} = \bar{y}$

-By setting $w_i = 1$ we could solve for $\hat{\lambda} = \bar{y}$

.

.

.

.

.

.

.

.

##Question 2##
```{r, echo = FALSE, results = 'hide'}
Data = read.csv("C:/Users/Luke\\Downloads/SingaporeAuto.csv")
```

The summary of the data doesn't really give us much information to go off of for the claim counts so we look at the histogram to get a better visualization of the data.

```{r, echo = FALSE, results = 'hide'}
n=length(Data$Clm_Count)


head(Data)
```

```{r, echo = FALSE}
summary(Data$Clm_Count)
summary(Data$Exp_weights)
par(mfrow=c(1,2))
H=hist(Data$Clm_Count,5, main="Claims Count")
hist(Data$Exp_weights, main="Exposures")
```

```{r, echo = FALSE}
# Claim counts
NrClaims=unique(Data$Clm_Count) 
Claims=c()
nn=length(NrClaims)
for(i in 1:nn){
Claims[i]=sum(Data$Clm_Count==NrClaims[i])
}
A=as.table(matrix(c(NrClaims, Claims), length(Claims),2))
colnames(A)=c("#Claims", "Observed")
rownames(A)=c("","","","")
A
```

###Implementing a Homogeneous Poisson Model
```{r, echo = FALSE}
# Fit Poisson distribution with and wihtout Exposures
mu=mean(Data$Clm_Count)
mu_E=sum(Data$Clm_Count)/sum(Data$Exp_weights)
Claims_Poisson=c()
Claims_Poisson_E=c()
for(i in 1:nn){
  k=NrClaims[i]
  Claims_Poisson[i]=n*exp(-mu)*mu^k/factorial(k)
  Claims_Poisson_E[i]=sum(exp(-mu_E*Data$Exp_weights)*(mu_E*Data$Exp_weights)^k/factorial(k))
}
options(digits=2)
Table=matrix(c(NrClaims, Claims,Claims_Poisson,Claims_Poisson_E), length(Claims),4)
colnames(Table)=c("#Claims", "Data","Model without Exposure","Model With Exposure"  )
rownames(Table)=c("","","","")
as.table(Table)

ChiSqSt=sum((Claims-Claims_Poisson)^2/Claims_Poisson)
ChiSqSt_E=sum((Claims-Claims_Poisson_E)^2/Claims_Poisson_E)
```

We can see that the model without exposure differs from the actual data observed. We can see that including exposures is necesarry for the model because it hones the values for each claim count closer to the observed values, but it is still not an adeqaute fit.

#####Chi Square Statistic without exposures:

```{r, echo = FALSE}
ChiSqSt
```

#####Chi Square Statistic with exposures:

```{r, echo = FALSE}
ChiSqSt_E
```

#####Don't Know what this one is:

```{r, echo = FALSE}
qchisq(0.95,4-1-1)
```

We now investigate modeling different risk parameters for different drivers based on factors involved.

###Generalized Linear Poisson Model

```{r, echo = FALSE, results = 'hide'}
#####Investigate personal characteristics
Data$Female=1*(Data$SexInsured=="F")
Data$Auto=1*(Data$VehicleType=="A")
Data$NCD1F=relevel(factor(Data$NCD), ref="50")
I=(Data$AgeCat==0)
Data$AgeCat[I]=7
Data$AgeCatF=relevel(factor(Data$AgeCat), ref="7")
Data$VAgeCat1F=relevel(factor(Data$VAgecat1), ref="6")

poisson =glm(Clm_Count ~ Female + Auto  + Auto:AgeCatF + Auto:NCD1F + VAgeCat1F, offset=LNWEIGHT, poisson(link = log), data = Data)
```

```{r, echo = FALSE, results = 'hide'}
#####Split data in Auto and Other
Data_Auto=Data[Data$VehicleType=="A",]
Data_Other=Data[Data$VehicleType!="A",]
n_Auto=length(Data_Auto$Clm_Count)
n_Other=length(Data_Other$Clm_Count)

Nr_Acc_Auto=sum(Data_Auto$Clm_Count)
Nr_Acc_Other=sum(Data_Other$Clm_Count)
Pr_Acc_Auto=Nr_Acc_Auto/n_Auto
Pr_Acc_Other=Nr_Acc_Other/n_Other
```

#####Number of claims based on vehicle type:
```{r, echo = FALSE}
NrClaims=unique(Data$Clm_Count) 
Claims_Auto=c()
Claims_Other=c()
nn=length(NrClaims)
for(i in 1:nn){
  Claims_Auto[i]=100*sum(Data_Auto$Clm_Count==NrClaims[i])/n_Auto
  Claims_Other[i]=100*sum(Data_Other$Clm_Count==NrClaims[i])/n_Other
}
A=as.table(matrix(c(NrClaims, Claims_Auto, Claims_Other), length(Claims_Auto),3))
colnames(A)=c("#Claims", "Auto", "Other")
rownames(A)=c("","","","")
A
```

#####Effects of vehicle age:
```{r, echo = FALSE}
NrClaims=unique(Data$Clm_Count) 
Claims_VCat1=c()
Claims_VCat2=c()
Claims_VCat3=c()
Claims_VCat4=c()
Claims_VCat5=c()
nn=length(NrClaims)

for(i in 1:nn){
  Claims_VCat1[i]=100*sum(Data$Clm_Count[Data$VAgeCat1==2]==NrClaims[i])/sum(Data$VAgecat1==2)
  Claims_VCat2[i]=100*sum(Data$Clm_Count[Data$VAgeCat1==3]==NrClaims[i])/sum(Data$VAgecat1==3)
  Claims_VCat3[i]=100*sum(Data$Clm_Count[Data$VAgeCat1==4]==NrClaims[i])/sum(Data$VAgecat1==4)
  Claims_VCat4[i]=100*sum(Data$Clm_Count[Data$VAgeCat1==5]==NrClaims[i])/sum(Data$VAgecat1==5)
  Claims_VCat5[i]=100*sum(Data$Clm_Count[Data$VAgeCat1==6]==NrClaims[i])/sum(Data$VAgecat1==6)
 }
A=as.table(matrix(c(NrClaims,  Claims_VCat1,  Claims_VCat2,  Claims_VCat3,  Claims_VCat4,  Claims_VCat5),  length( Claims_VCat1),6))
colnames(A)=c("#Claims", "Age Cat 1: 0-2", "Age Cat 2: 3-5", "Age Cat 3: 6-10", "Age Cat 4: 11-15", "Age Cat 5: 16 and older")
rownames(A)=c()
A
```

#####Effects on age of insured:
```{r, echo = FALSE}
NrClaims=unique(Data$Clm_Count) 
Claims_AgeCat1=c()
Claims_AgeCat2=c()
Claims_AgeCat3=c()
Claims_AgeCat4=c()
Claims_AgeCat5=c()
Claims_AgeCat6=c()

nn=length(NrClaims)

for(i in 1:nn){
  Claims_AgeCat1[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$AgeCat==2]==NrClaims[i])/sum(Data_Auto$AgeCat==2)
  Claims_AgeCat2[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$AgeCat==3]==NrClaims[i])/sum(Data_Auto$AgeCat==3)
  Claims_AgeCat3[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$AgeCat==4]==NrClaims[i])/sum(Data_Auto$AgeCat==4)
  Claims_AgeCat4[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$AgeCat==5]==NrClaims[i])/sum(Data_Auto$AgeCat==5)
  Claims_AgeCat5[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$AgeCat==6]==NrClaims[i])/sum(Data_Auto$AgeCat==6)
  Claims_AgeCat6[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$AgeCat==7]==NrClaims[i])/sum(Data_Auto$AgeCat==7)
}
A=as.table(matrix(c(NrClaims,  Claims_AgeCat1,  Claims_AgeCat2,  Claims_AgeCat3,  Claims_AgeCat4,  Claims_AgeCat5, Claims_AgeCat6),  length( Claims_AgeCat1),7))
colnames(A)=c("#Claims", "Age Cat 1: 22-25", "Age Cat 2: 26-35", "Age Cat 3: 36-45", "Age Cat 4: 46-55", "Age Cat 5: 56-65", "Age Cat 6: 66 and over")
rownames(A)=c()
A
```

#####Effects of the no claims discount:
```{r, echo = FALSE}
NrClaims=unique(Data$Clm_Count) 
Claims_NCD1=c()
Claims_NCD2=c()
Claims_NCD3=c()
Claims_NCD4=c()
Claims_NCD5=c()
Claims_NCD6=c()

nn=length(NrClaims)

for(i in 1:nn){
  Claims_NCD1[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$NCD==0]==NrClaims[i])/sum(Data_Auto$NCD==0)
  Claims_NCD2[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$NCD==10]==NrClaims[i])/sum(Data_Auto$NCD==10)
  Claims_NCD3[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$NCD==20]==NrClaims[i])/sum(Data_Auto$NCD==20)
  Claims_NCD4[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$NCD==30]==NrClaims[i])/sum(Data_Auto$NCD==30)
  Claims_NCD5[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$NCD==40]==NrClaims[i])/sum(Data_Auto$NCD==40)
  Claims_NCD6[i]=100*sum(Data_Auto$Clm_Count[Data_Auto$NCD==50]==NrClaims[i])/sum(Data_Auto$NCD==50)
}
A=as.table(matrix(c(NrClaims,  Claims_NCD1,  Claims_NCD2,  Claims_NCD3,  Claims_NCD4,  Claims_NCD5, Claims_NCD6),  length( Claims_AgeCat1),7))
colnames(A)=c("#Claims", "0", "10", "20", "30", "40", "50")
rownames(A)=c()
A
```

```{r, echo = FALSE}
CountGLM_P=glm(Clm_Count ~ Female + Auto  + Auto:AgeCatF + Auto:NCD1F + VAgeCat1F, offset=LNWEIGHT, poisson(link=log), data= Data)
```

#####Comparing the models
```{r, echo = FALSE}
NrClaims=unique(Data$Clm_Count) 

mu=mean(Data$Clm_Count)
mu_E=sum(Data$Clm_Count)/sum(Data$Exp_weights)
lambdaHat_GLM=exp(predict(CountGLM_P))

Claims=c()
Claims_Poisson=c()
Claims_Poisson_E=c()
Claims_Poisson_GLM=c()

for(i in 1:length(NrClaims)){
  k=NrClaims[i]
  Claims[i]=sum(Data$Clm_Count==NrClaims[i])
  
  Claims_Poisson[i]=n*exp(-mu)*mu^k/factorial(k)
  Claims_Poisson_E[i]=sum(exp(-mu_E*Data$Exp_weights)*(mu_E*Data$Exp_weights)^k/factorial(k))
  Claims_Poisson_GLM[i]=sum(exp(-lambdaHat_GLM)*(lambdaHat_GLM)^k/factorial(k))
}
options(digits=2)
Table=matrix(c(NrClaims, Claims,Claims_Poisson,Claims_Poisson_E,  Claims_Poisson_GLM), length(Claims),5)
colnames(Table)=c("#Claims", "Data","Model without Exposure","Model With Exposure", "Poisson Regression"  )
as.table(Table)
```

###Fit Negative Binomial
```{r, echo = FALSE}
library(MASS)
binomial = glm.nb(Data$Clm_Count ~ Data$Exp_weights, data = Data)
summary(binomial)
```

###Proofs
$E[N] = E[E[N|\theta]] = E[\theta\lambda] = \lambda E[\theta] = \lambda*1 = \lambda$

$Var[N] = E[N^2] - [E[N]]^2 = E[E[N^2|\theta]] - \lambda^2$


overdispersion is when there is a presence of a greater variability than should be expected.

```{r}
functn = function(Par, N, E) {
  lamb = Par[1]
  gam = Par[2]
NB = -sum(log((gamma(N+gam))/(gamma(gam)*factorial(N))*((1-((lamb * E)/(gam + (lamb * E))))^gam)*(((lamb * E)/(gam + (lamb * E)))^N)))
return(NB)
}

param = c(1, 1)
functn(Par = param, N = Data$Clm_Count, E = Data$Exp_weights)
opt = optim(par = param, fn = functn, N = Data$Clm_Count, E = Data$Exp_weights)
opt
Lambda = opt$par[1]
Gamma = opt$par[2]

z = matrix(data = NA, nrow = 40, ncol = 40)
x = seq(0.01, 0.4, 0.01)
y = seq(.04, 1.6, 0.04)
for(i in 1:length(x)){
  for(j in 1:length(y)){
    vector = c(x[i], y[j])
  z[i, j] = functn(Par = vector, N = Data$Clm_Count, E =   Data$Exp_weights)
  }
}
persp(x, y, z)
##By the persp plot we can see that there is only in maximum and one minimum, so the optim values of the maximum values are the absolute maximums.
```

```{r}
Claims_NB_E=c()
for(i in 1:length(NrClaims)){
  N = unique(NrClaims[i])
  Claims_NB_E[i]=sum((gamma(N+Gamma))/(gamma(Gamma)*factorial(N))*((1-((Lambda * Data$Exp_weights)/(Gamma + (Lambda * Data$Exp_weights))))^Gamma)*(((Lambda * Data$Exp_weights)/(Gamma + (Lambda * Data$Exp_weights)))^N))
  }

options(digits=2)
Table=matrix(c(NrClaims, Claims,Claims_NB_E), length(Claims),3)
colnames(Table)=c("#Claims", "Data", "Model With Exposure"  )
rownames(Table)=c("","","","")
as.table(Table)

## Comparing the models
options(digits=2)
Table=matrix(c(NrClaims, Claims, Claims_Poisson_GLM, Claims_NB_E), length(Claims),4)
colnames(Table)=c("#Claims", "Data", "Poisson Regression", "Negative Binomial"  )
as.table(Table)
## ==> We can see that the Negative Binomial is much more precise and closer to the actual data values than the Poisson Regression model is.

##Chi Squares
ChiSqSt_NB_E=sum((Claims-Claims_NB_E)^2/Claims_NB_E)
ChiSqSt_NB_E
ChiSqST_Poisson_GLM=sum((Claims-Claims_Poisson_GLM)^2/Claims_Poisson_GLM)
ChiSqST_Poisson_GLM
## ==> The Chi Square statistic for the Negative Binomial is much smaller than the Poisson Regression model's statistic which is evidence that the Negative Binomial differs much less from the actual data than Poisson.
```



# Estimated Dispersion
Dispersion_P=sum(residuals(poisson, type="pearson")^2)/df.residual(poisson)
Dispersion_NB=sum(residuals(binomial, type="pearson")^2)/df.residual(binomial)


###Introduction
The car insurance company has already implemented a Poisson regression model to describe the claim counts distribution. I have been hired to test whether the Poisson model should be replaced by a homogeneous negative binomial model. I have concluded that a poisson regression model better fits the data and should be used to describe the claim counts distribution.

###Methods
Linders Consulting is implementing a Poisson regression model to describe the claim counts distribution. Because $\lambda$ is the parameter of a Poisson distribution and is the value for both the mean and variance, it may be inaccurate to assume all samples follow the same $\lambda$ variable. When the mean and variance are not equal in a counts distribution, there is an under or overdispersion, depending on if the variance is smaller or larger than the mean. The issue could be resolved by implementing a homogeneous negative binomial regression model. The variance is $\lambda(1+\lambda/\gamma)$ and mean of $\lambda$.

###Results
The homogeneous negative binomial has a $\theta$ of 1.4464912. The estimated values of dispersion for the Poisson and homogeneous negative binomial model are 0.996757 and 0.9727552, respectiveyl. They are both close to 1 and can be described as the residual being normal, however the Poisson dispersion is more normal than the negative binomial dispersion making it a better fit.

###Conclusion
The Poisson distribution will be the better predicttion model.